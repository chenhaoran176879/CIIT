from transformers import AutoModelForCausalLM, AutoTokenizer
from collections import defaultdict
from video_summarization_prompt import video_summarization_prompt

import json

device = "cuda" # the device to load the model onto





def load_processed_videos(output_file):
    """
    读取已经处理过的视频信息并返回一个集合，集合中存储视频文件名或者索引
    """
    processed_videos = set()
    try:
        with open(output_file, 'r') as f:
            for line in f:
                result = json.loads(line.strip())
                processed_videos.add(result["video_name"])
                # 如果你希望用索引来记录，使用 result["index"] 替换 result["video_name"]
    except FileNotFoundError:
        print(f"{output_file} 文件不存在，将从头开始处理。")
    except Exception as e:
        print(f"读取 {output_file} 时发生错误: {e}")
    return processed_videos


def load_json(file_path):
    data = []
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            json_obj = json.loads(line)
            simplified_data = {}
            simplified_data['video_name'] = json_obj['video_name']
            simplified_data['index'] = json_obj['index']
            simplified_data['tag'] = json_obj['video_name'][:-8]
            simplified_data['description'] = json.loads(json_obj['response']).get('description',None)
            data.append(simplified_data)
    return data



check_classification_prompt = '''Here are a classification tag and a description of a video of crime.
The classification tag is ground truth and the description is generated by AI.
I found there might be some mistakes of AI's answer.
For example, the tag is Shooting but the description is unclear about shooting.
Please decide whether they match and give your answer following my examples below.

Example 1:
Tag: Shooting
Description: A sudden outbreak of violence occurred in the video. Initially, five individuals were seen conversing, but later they engaged in a physical altercation. A woman approached to witness the fight.
Your answer: No. The description is about violence and physical fighting, and shooting is not related.

Example 2:
Tag: Robbery
Description: An armed robbery and subsequent explosions occurred in the video. Initially, three gunmen entered, causing the staff to flee. Two robbers initially left, but one returned and intimidated a clerk. The gangsters looted the store, with one forcing the clerk to the ground. In the midst of the robbery, there were three distinct explosions at different intervals, and despite the chaos, a gangster managed to escape with two boxes of stolen items.
Your answer: Yes. The tag and the description both mentioned robbery.


Question for you to answer:
Tag: {}
Description: {}
Your answer: 
'''

if __name__ == '__main__':
    file_path = "/mnt/lustre/chenhaoran/CIIT/LLaVA-NeXT-inference/playground/ucfcrime/slurm_log/LLM_summarization_results.jsonl"
    dataset = load_json(file_path=file_path)
    output_file = '/mnt/lustre/chenhaoran/CIIT/LLaVA-NeXT-inference/playground/ucfcrime/slurm_log/LLM_summarization_examination.jsonl'
    processed_videos = load_processed_videos(output_file)
    print(f"已处理视频数量: {len(processed_videos)}")

    model = AutoModelForCausalLM.from_pretrained(
        "/home/share/chenhaoran/model_zoo/Qwen1.5-72B-Chat-GPTQ-Int4/",
        torch_dtype="auto",
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained("/home/share/chenhaoran/model_zoo/Qwen1.5-72B-Chat-GPTQ-Int4/")
    device = model.device

    with open(output_file, 'a') as f:
        for i in range(0, len(dataset)):
            data = dataset[i]
            video_name = data['video_name']

            if video_name in processed_videos:
                print(f"Skipping already processed video: {video_name}")
                continue

            if video_name.lower().startswith('normal'):
                print(f"Skipping normal data at index {i}: {video_name}")
                continue

            print(f"Processing index {i}, video name: {video_name}")


            messages = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content":check_classification_prompt.format(data['tag'],data['description'])}
            ]

            print(f"Messages for index {i}: {messages}")

            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            model_inputs = tokenizer([text], return_tensors="pt").to(device)

            generated_ids = model.generate(
                model_inputs.input_ids,
                max_new_tokens=512
            )
            generated_ids = [
                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]

            response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

            print(f"Generated response for index {i}: {response}")

            result = {
                "index": data['index'],
                "video_name": video_name,
                "messages": messages,
                "response": response
            }

            f.write(json.dumps(result) + '\n')
            f.flush()

            processed_videos.add(video_name)
