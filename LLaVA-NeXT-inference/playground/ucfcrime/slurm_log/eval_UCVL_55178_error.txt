The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/11 [00:00<?, ?it/s]Loading checkpoint shards:   9%|▉         | 1/11 [00:20<03:27, 20.71s/it]Loading checkpoint shards:  18%|█▊        | 2/11 [00:38<02:51, 19.07s/it]Loading checkpoint shards:  27%|██▋       | 3/11 [00:52<02:11, 16.47s/it]Loading checkpoint shards:  36%|███▋      | 4/11 [01:04<01:44, 14.97s/it]Loading checkpoint shards:  45%|████▌     | 5/11 [01:16<01:24, 14.00s/it]Loading checkpoint shards:  55%|█████▍    | 6/11 [01:30<01:08, 13.77s/it]Loading checkpoint shards:  64%|██████▎   | 7/11 [01:43<00:54, 13.57s/it]Loading checkpoint shards:  73%|███████▎  | 8/11 [01:56<00:40, 13.34s/it]Loading checkpoint shards:  82%|████████▏ | 9/11 [02:10<00:27, 13.62s/it]Loading checkpoint shards:  91%|█████████ | 10/11 [02:24<00:13, 13.66s/it]Loading checkpoint shards: 100%|██████████| 11/11 [02:27<00:00, 10.53s/it]Loading checkpoint shards: 100%|██████████| 11/11 [02:27<00:00, 13.43s/it]
Token indices sequence length is longer than the specified maximum sequence length for this model (8489 > 8192). Running this sequence through the model will result in indexing errors
/mnt/lustre/chenhaoran/anaconda3/envs/llava-interleave/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
