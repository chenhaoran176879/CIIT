Unrecognized keys in `rope_scaling` for 'rope_type'='dynamic': {'type'}
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   6%|▌         | 1/17 [00:19<05:06, 19.14s/it]Loading checkpoint shards:  12%|█▏        | 2/17 [00:34<04:10, 16.69s/it]Loading checkpoint shards:  18%|█▊        | 3/17 [00:45<03:22, 14.49s/it]Loading checkpoint shards:  24%|██▎       | 4/17 [00:55<02:41, 12.40s/it]Loading checkpoint shards:  29%|██▉       | 5/17 [01:05<02:19, 11.66s/it]Loading checkpoint shards:  35%|███▌      | 6/17 [01:15<02:00, 10.98s/it]Loading checkpoint shards:  41%|████      | 7/17 [01:25<01:46, 10.66s/it]Loading checkpoint shards:  47%|████▋     | 8/17 [01:35<01:35, 10.61s/it]Loading checkpoint shards:  53%|█████▎    | 9/17 [01:47<01:26, 10.86s/it]Loading checkpoint shards:  59%|█████▉    | 10/17 [01:55<01:11, 10.25s/it]Loading checkpoint shards:  65%|██████▍   | 11/17 [02:06<01:02, 10.38s/it]Loading checkpoint shards:  71%|███████   | 12/17 [02:17<00:53, 10.64s/it]Loading checkpoint shards:  76%|███████▋  | 13/17 [02:28<00:42, 10.50s/it]Loading checkpoint shards:  82%|████████▏ | 14/17 [02:37<00:30, 10.22s/it]Loading checkpoint shards:  88%|████████▊ | 15/17 [02:46<00:19,  9.93s/it]Loading checkpoint shards:  94%|█████████▍| 16/17 [02:56<00:09,  9.92s/it]Loading checkpoint shards: 100%|██████████| 17/17 [03:01<00:00,  8.33s/it]Loading checkpoint shards: 100%|██████████| 17/17 [03:01<00:00, 10.67s/it]
Token indices sequence length is longer than the specified maximum sequence length for this model (8517 > 8192). Running this sequence through the model will result in indexing errors
/mnt/lustre/chenhaoran/anaconda3/envs/llava-interleave/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
